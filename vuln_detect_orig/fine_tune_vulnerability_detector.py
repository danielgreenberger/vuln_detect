"""
Vulnerability Detection Fine-Tuning Script
Fine-tunes a transformer model (RoBERTa) to detect self-harm risk levels in social media posts.

Risk Levels:
- 0: Neutral (No depression detected)
- 1: Moderate (Person would benefit from help, no immediate danger)
- 2: Severe (Person is in danger)
"""

import torch
from datasets import Dataset, DatasetDict
from transformers import (
    AutoTokenizer,
    DataCollatorWithPadding,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
)
import numpy as np
import evaluate
from pathlib import Path
import json
import pandas as pd

print("CUDA available:", torch.cuda.is_available())
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")

# ========================================================================
# Configuration
# ========================================================================
class Config:
    """
    Configuration for fine-tuning
    
    Memory optimization for M1/M2 Macs:
    - Ultra-low batch_size=4 with gradient_accumulation_steps=4
      gives effective batch size of 16
    - Reduced max_length=256 to use less memory (from 512)
    - Use --max-samples for testing (e.g., 5000-10000)
    - Close other apps to free memory
    - If MPS still causes OOM, use CPU:
      python fine_tune_vulnerability_detector.py --data ... --device cpu
    """
    # Model configuration
    model_name = "cardiffnlp/twitter-roberta-base"  # Better for social media text
    num_labels = 3  # Neutral, Moderate, Severe
    
    # Training configuration
    output_dir = "vulnerability_detector_model"
    learning_rate = 2e-5
    
    # Ultra-low batch sizes for M1/M2 Mac compatibility
    per_device_train_batch_size = 4  # Reduced from 8
    per_device_eval_batch_size = 4   # Reduced from 8
    
    # Increased gradient accumulation to maintain effective batch size of 16
    gradient_accumulation_steps = 4  # Increased from 2
    
    num_train_epochs = 3
    weight_decay = 0.01
    save_strategy = "epoch"
    evaluation_strategy = "epoch"
    logging_steps = 100
    
    # Data configuration
    max_length = 256  # Reduced from 512 to save memory
    train_test_split = 0.2
    
    # Label mapping
    label_names = ["Neutral", "Moderate", "Severe"]
    id2label = {0: "Neutral", 1: "Moderate", 2: "Severe"}
    label2id = {"Neutral": 0, "Moderate": 1, "Severe": 2}

# dgreenbe TODO: whats Attention Masks?
# ========================================================================
# Step 1: Data Loading
# ========================================================================
def load_reddit_data(data_path=None, max_samples=None):
    """
    Load Reddit posts data from CSV/JSON or create sample data
    
    Expected format:
    - 'text': The post content
    - 'label': Risk level (0=Neutral, 1=Moderate, 2=Severe)
    - 'subreddit' (optional): Source subreddit
    """
    if data_path and not Path(data_path).exists():
        raise ValueError(f"Input file {data_path} doesn't exist!")
    
    # Load from CSV or JSON
    if data_path.endswith('.csv'):
        df = pd.read_csv(data_path)
    elif data_path.endswith('.json'):
        df = pd.read_json(data_path)
    else:
        raise ValueError("Data file must be CSV or JSON")
    
    # Limit samples if specified (useful for testing on smaller hardware)
    if max_samples is not None:
        print(f"Limiting dataset to {max_samples} samples for testing")
        df = df.sample(n=min(max_samples, len(df)), random_state=42)
        
    print(f"Loaded {len(df)} samples from {data_path}")
    
    # Validate required columns
    if 'text' not in df.columns or 'label' not in df.columns:
        raise ValueError("Data must contain 'text' and 'label' columns")
        
    return df


def prepare_datasets(df, test_size=0.2):
    """Convert pandas DataFrame to HuggingFace Dataset with train/val/test splits"""
    # Create dataset from pandas
    dataset = Dataset.from_pandas(df[['text', 'label']])
    
    # Split into train/val/test (70/15/15)
    # Note: stratify_by_column may not be available in all versions of datasets
    # Using simple random split with fixed seed for reproducibility
    train_test_split = dataset.train_test_split(
        test_size=0.3,  # 30% for val+test
        seed=42
    )
    train_dataset = train_test_split['train']  # 70%

    # Further split the 30% into validation (15%) and test (15%)
    val_test_split = train_test_split['test'].train_test_split(
        test_size=0.5,  # Split 30% in half
        seed=42
    )
    val_dataset = val_test_split['train']  # 15% of total
    test_dataset = val_test_split['test']   # 15% of total

    print(f"\nDataset splits:")
    print(f"  Train: {len(train_dataset)} samples ({len(train_dataset)/len(dataset)*100:.1f}%)")
    print(f"  Validation: {len(val_dataset)} samples ({len(val_dataset)/len(dataset)*100:.1f}%)")
    print(f"  Test: {len(test_dataset)} samples ({len(test_dataset)/len(dataset)*100:.1f}%)")

    # Print class distribution for each split to verify balance
    for split_name, split_data in [('Train', train_dataset), ('Validation', val_dataset), ('Test', test_dataset)]:
        labels = split_data['label']
        class_counts = {}
        for label in labels:
            class_counts[label] = class_counts.get(label, 0) + 1
        print(f"  {split_name} class distribution: {class_counts}")
    
    return DatasetDict({
        'train': train_dataset,
        'validation': val_dataset,
        'test': test_dataset
    })

# ========================================================================
# Step 2: Tokenization
# ========================================================================
def tokenize_data(dataset, tokenizer):
    """Tokenize the text data"""
    def preprocess_function(examples):
        return tokenizer(
            examples["text"],
            truncation=True,
            max_length=Config.max_length,
            padding=False  # Will be done dynamically by DataCollator
        )
    
    print("\nTokenizing datasets...")
    # Tokenize in batches for memory efficiency
    # batched=True processes multiple examples at once, reducing memory overhead
    # remove_columns removes original text after tokenization to save memory
    # Get column names from one of the splits (they're all the same)
    columns_to_remove = list(dataset['train'].column_names)
    # Keep only 'label' column, remove 'text' and any index columns
    columns_to_remove = [col for col in columns_to_remove if col != 'label']
    
    tokenized_dataset = dataset.map(
        preprocess_function,
        batched=True,
        remove_columns=columns_to_remove
    )
    
    return tokenized_dataset

# ========================================================================
# Step 3: Model and Training Setup
# ========================================================================
def setup_model(model_name=None):
    """Initialize the model for sequence classification"""
    if model_name is None:
        model_name = Config.model_name
    
    print(f"\nLoading model: {model_name}")
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name,
        num_labels=Config.num_labels,
        id2label=Config.id2label,
        label2id=Config.label2id,
    )
    
    return model

def setup_metrics():
    """Setup evaluation metrics"""
    accuracy_metric = evaluate.load("accuracy")
    f1_metric = evaluate.load("f1")
    precision_metric = evaluate.load("precision")
    recall_metric = evaluate.load("recall")
    
    def compute_metrics(eval_pred):
        logits, labels = eval_pred
        predictions = np.argmax(logits, axis=-1)
        
        accuracy = accuracy_metric.compute(predictions=predictions, references=labels)["accuracy"]
        f1 = f1_metric.compute(predictions=predictions, references=labels, average="weighted")["f1"]
        precision = precision_metric.compute(predictions=predictions, references=labels, average="weighted")["precision"]
        recall = recall_metric.compute(predictions=predictions, references=labels, average="weighted")["recall"]
        
        return {
            "accuracy": accuracy,
            "f1": f1,
            "precision": precision,
            "recall": recall
        }
    
    return compute_metrics

# ========================================================================
# Main Training Pipeline
# ========================================================================
def main(data_path=None, max_samples=None, device='auto'):
    """Main training pipeline"""
    
    # Force CPU if requested (must be done before any torch operations)
    if device == 'cpu':
        import os
        os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"
        os.environ["CUDA_VISIBLE_DEVICES"] = ""
        print("=" * 70)
        print("Environment configured for CPU-only training")
        print("MPS and CUDA disabled via environment variables")
        print("=" * 70)
    
    # Step 1: Load data
    print("=" * 70)
    print("STEP 1: Loading Data")
    print("=" * 70)
    df = load_reddit_data(data_path, max_samples=max_samples)
    dataset = prepare_datasets(df, test_size=Config.train_test_split)
    
    # Step 2: Initialize tokenizer
    print("\n" + "=" * 70)
    print("STEP 2: Tokenization")
    print("=" * 70)
    tokenizer = AutoTokenizer.from_pretrained(Config.model_name)
    tokenized_dataset = tokenize_data(dataset, tokenizer)
    
    # Step 3: Setup data collator
    print("\n" + "=" * 70)
    print("STEP 3: Data Collator Setup")
    print("=" * 70)
    # Explanation: DataCollatorWithPadding dynamically pads sequences to the same length within each batch (to the longest sequence in that batch).
    # This is more efficient than padding everything to the global max length (512 tokens), saving computation on shorter texts.
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer) # dgreenbe TODO: what's that?
    print("Data collator initialized (dynamic padding)")
    
    # Step 4: Determine device
    print("\n" + "=" * 70)
    print("STEP 4: Device Selection")
    print("=" * 70)
    if device == 'cpu':
        selected_device = 'cpu'
        print("Using CPU (slower but more reliable for large models)")
    elif device == 'mps':
        selected_device = 'mps' if torch.backends.mps.is_available() else 'cpu'
        print(f"Using device: {selected_device}")
    else:  # auto
        if torch.backends.mps.is_available():
            selected_device = 'mps'
            print("Using MPS (Metal Performance Shaders - Mac GPU)")
            print("Note: If you get OOM errors, try: --device cpu")
        elif torch.cuda.is_available():
            selected_device = 'cuda'
            print("Using CUDA GPU")
        else:
            selected_device = 'cpu'
            print("Using CPU")
    
    # Step 5: Initialize model
    print("\n" + "=" * 70)
    print("STEP 5: Model Initialization")
    print("=" * 70)
    print(f"Model: {Config.model_name}")
    print(f"Number of labels: {Config.num_labels}")
    model = setup_model()
    
    # Enable gradient checkpointing to save memory
    if hasattr(model, 'gradient_checkpointing_enable'):
        model.gradient_checkpointing_enable()
        print("Gradient checkpointing enabled (trades speed for memory)")
    
    # Move model to selected device
    model = model.to(selected_device)
    print(f"Model moved to device: {selected_device}")
    
    # Step 6: Setup metrics
    print("\n" + "=" * 70)
    print("STEP 6: Metrics Setup")
    print("=" * 70)
    compute_metrics = setup_metrics()
    print("Metrics initialized: accuracy, f1, precision, recall")
    
    # Step 7: Configure training arguments
    print("\n" + "=" * 70)
    print("STEP 7: Training Configuration")
    print("=" * 70)
    
    # Configure training arguments with device-specific settings
    training_args_dict = {
        "output_dir": Config.output_dir,
        "learning_rate": Config.learning_rate,
        "per_device_train_batch_size": Config.per_device_train_batch_size,
        "per_device_eval_batch_size": Config.per_device_eval_batch_size,
        "gradient_accumulation_steps": Config.gradient_accumulation_steps,
        "num_train_epochs": Config.num_train_epochs,
        "weight_decay": Config.weight_decay,
        "save_strategy": Config.save_strategy,
        "eval_strategy": Config.evaluation_strategy,
        "logging_steps": Config.logging_steps,
        "load_best_model_at_end": True,
        "metric_for_best_model": "f1",
        "fp16": False,  # Disable FP16 for MPS compatibility
        "dataloader_pin_memory": False,  # Disable pin memory for MPS
        "gradient_checkpointing": True,  # Gradient checkpointing to save memory (slight speed tradeoff)
    }
    
    # Force CPU usage if requested
    if device == 'cpu' or selected_device == 'cpu':
        training_args_dict["use_cpu"] = True
        training_args_dict["no_cuda"] = True
        print("Forcing CPU training (MPS and CUDA disabled in TrainingArguments)")
    
    training_args = TrainingArguments(**training_args_dict)
    
    print(f"Output directory: {Config.output_dir}")
    print(f"Learning rate: {Config.learning_rate}")
    print(f"Batch size: {Config.per_device_train_batch_size}")
    print(f"Epochs: {Config.num_train_epochs}")
    print(f"Device mode: {device} (selected: {selected_device})")
    
    # Step 8: Initialize trainer
    print("\n" + "=" * 70)
    print("STEP 8: Trainer Initialization")
    print("=" * 70)
    # Note: Using validation set for evaluation during training
    # Test set is reserved for final evaluation only
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset["train"],
        eval_dataset=tokenized_dataset["validation"],  # Use validation for training evaluation
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
    )
    
    # Step 9: Train
    print("\n" + "=" * 70)
    print("STEP 9: Training")
    print("=" * 70)
    print("Starting training...")
    trainer.train()
    
    # Step 10: Evaluate
    print("\n" + "=" * 70)
    print("STEP 10: Final Evaluation")
    print("=" * 70)
    metrics = trainer.evaluate()
    
    print("\nFinal Evaluation Metrics:")
    for metric_name, metric_value in metrics.items():
        print(f"  {metric_name}: {metric_value:.4f}")
    
    # Step 11: Save model
    print("\n" + "=" * 70)
    print("STEP 11: Saving Model")
    print("=" * 70)
    trainer.save_model(Config.output_dir)
    tokenizer.save_pretrained(Config.output_dir)
    
    # Save training metadata (separate from model config)
    training_metadata = {
        'model_name': Config.model_name,
        'num_labels': Config.num_labels,
        'id2label': Config.id2label,
        'label2id': Config.label2id,
        'label_names': Config.label_names,
        'final_metrics': metrics
    }
    
    # Save to a separate file to not overwrite model's config.json
    with open(f"{Config.output_dir}/training_metadata.json", 'w') as f:
        json.dump(training_metadata, f, indent=2)
    
    print(f"Model saved to: {Config.output_dir}")
    print("\nTraining complete!")
    
    return trainer, metrics

# ========================================================================
# Entry Point
# ========================================================================
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Fine-tune transformer for vulnerability detection")
    parser.add_argument("--data", type=str, default=None,
                       help="Path to data file (CSV or JSON)")
    parser.add_argument("--max-samples", type=int, default=None,
                       help="Limit dataset to N samples (for M2 Mac try 5000-10000, not 50K)")
    parser.add_argument("--model", type=str, default=None,
                       help="Model name or path (default: cardiffnlp/twitter-roberta-base)")
    parser.add_argument("--epochs", type=int, default=None,
                       help="Number of training epochs")
    parser.add_argument("--batch-size", type=int, default=None,
                       help="Batch size for training")
    parser.add_argument('--device', type=str, default='auto',
                       choices=['auto', 'cpu', 'mps'],
                       help='Device to use: auto (default), cpu (slower but reliable), or mps (Mac GPU)')
    
    args = parser.parse_args()
    
    # Override config if arguments provided
    if args.model:
        Config.model_name = args.model
    if args.epochs:
        Config.num_train_epochs = args.epochs
    if args.batch_size:
        Config.per_device_train_batch_size = args.batch_size
        Config.per_device_eval_batch_size = args.batch_size
    
    # Run training
    trainer, metrics = main(data_path=args.data, max_samples=args.max_samples, device=args.device)